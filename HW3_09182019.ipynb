{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HW3 9/18/19\n",
    "# For next Thursday, I suggest we experiment with changing the size\n",
    "# of the minibatches over time.   We shall all train with 1,000\n",
    "# minibatches, but some groups will use minibatches of a fixed size and\n",
    "# others minibatches that start small and grow larger.  For the fixed\n",
    "# size, use minibatches of size 128 for a total of 128,000 observations. \n",
    "# For the variable size, start with 100 minibatches of size 32, then 150\n",
    "# minibatches of size 64, then 200 minibatches of size 128, then 250\n",
    "# minibatches of size 256, then 50 of size 512, for a total of 128,000\n",
    "# observations.\n",
    "\n",
    "#       Each group should use ReLU with two hidden layers of size 1024\n",
    "# with inputs and outputs standardized to lie between 0 and 1. All teams\n",
    "# will use the cross-entropy loss function.  There will be no weight\n",
    "# decay. We shall use the five data sets from the first exercise:  MNIST\n",
    "# digit (a), MNIST fashion (b), Cat/NonCat (c), Cifar-10 (d), and Street\n",
    "# View Digits (e).\n",
    "\n",
    "#       The team assignments are as follows:\n",
    "\n",
    "# If your team number ends in 1 in the table that is attached, use dataset\n",
    "# d with the fixed minibatch size.\n",
    "\n",
    "# If it ends in 2, use dataset e and fixed size.\n",
    "\n",
    "# If 3, use a and fixed.\n",
    "\n",
    "# 4 uses b and fixed.\n",
    "\n",
    "# 5 uses c and fixed.  ##Group 25\n",
    "\n",
    "# 6 uses d and increasing minibatch size.\n",
    "\n",
    "# 7 uses e and increasing size.\n",
    "\n",
    "# 8 uses a and increasing size.\n",
    "\n",
    "# 9 uses b and increasing size.\n",
    "\n",
    "# 0 uses c and increasing size.\n",
    "\n",
    "# Please report your final accuracies on the test sets in a file labeled\n",
    "# hw3(group number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up seed\n",
    "#https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "from numpy.random import seed\n",
    "seed(23)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "(209,)\n",
      "(50, 64, 64, 3)\n",
      "(50,)\n",
      "[b'non-cat' b'cat']\n"
     ]
    }
   ],
   "source": [
    "#3. Cat and Non-Cat Dataset\n",
    "import numpy as np\n",
    "import h5py\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5',\"r\")\n",
    "    train_set_x_orig=np.array(train_dataset[\"train_set_x\"][:]) #your train set features\n",
    "    train_set_y_orig=np.array(train_dataset[\"train_set_y\"][:]) #your train set labels\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5',\"r\")\n",
    "    test_set_x_orig=np.array(test_dataset[\"test_set_x\"][:]) #your test set features\n",
    "    test_set_y_orig=np.array(test_dataset[\"test_set_y\"][:]) #your test set labels\n",
    "    classes=np.array(test_dataset[\"list_classes\"][:]) #the list of classes\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "x_train, y_train, x_test, y_test, classes = load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 209)\n",
      "209\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "a=range(209)\n",
    "print(a)\n",
    "idx=choices(a, k=128000)\n",
    "print(len(np.unique(idx)))\n",
    "print(len(idx))\n",
    "\n",
    "# np.random.choice(209, 128, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128000, 64, 64, 3)\n",
      "(128000,)\n"
     ]
    }
   ],
   "source": [
    "x_train_final=x_train[idx,]\n",
    "y_train_final=y_train[idx,]\n",
    "print(x_train_final.shape)\n",
    "print(y_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0924 10:31:12.610924 4504733120 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "128000/128000 [==============================] - 252s 2ms/step - loss: 5.5394 - acc: 0.6558\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "test_acc: 0.34\n",
      "test_loss: 10.637942962646484\n",
      "Weights for 1st hidden layer\n",
      "[[ 0.01016974 -0.01379796  0.01061412 ... -0.00522971  0.0202475\n",
      "  -0.01790311]\n",
      " [-0.01207365  0.02136869  0.0170865  ... -0.01048139 -0.01655451\n",
      "  -0.01044076]\n",
      " [-0.00136691 -0.00813045 -0.01849877 ... -0.00307082 -0.01423372\n",
      "   0.01651389]\n",
      " ...\n",
      " [-0.00325957 -0.0116712  -0.01863468 ... -0.00719265 -0.01503647\n",
      "  -0.01136483]\n",
      " [-0.00640188 -0.00522094  0.01224651 ... -0.00610555  0.01528105\n",
      "   0.02103283]\n",
      " [-0.005223    0.01015245 -0.00238091 ... -0.0162192  -0.01059548\n",
      "  -0.01455467]]\n",
      "Intercept for 1st hidden layer\n",
      "[-0.00316137  0.00315993 -0.00316021 ...  0.00316216  0.00316183\n",
      "  0.00316151]\n",
      "Weights for 2nd hidden layer\n",
      "[[-0.00395939 -0.0266795  -0.02352583 ... -0.03481086  0.03443879\n",
      "  -0.02720722]\n",
      " [ 0.0417206   0.00205107  0.05648429 ... -0.01976223 -0.03709526\n",
      "   0.01413536]\n",
      " [ 0.00702633 -0.04901979 -0.04965963 ... -0.03383405  0.04706069\n",
      "   0.00743938]\n",
      " ...\n",
      " [-0.01133313 -0.02399562 -0.03145672 ...  0.00296221 -0.0392643\n",
      "  -0.04370802]\n",
      " [ 0.01126857 -0.01349472  0.01082246 ...  0.04267578  0.00655309\n",
      "  -0.01984439]\n",
      " [-0.01407673 -0.01167791 -0.04085886 ... -0.01271161 -0.01314039\n",
      "  -0.02744474]]\n",
      "Intercept for 1st hidden layer\n",
      "[ 0.00316205  0.00315377 -0.00316192 ... -0.00315802 -0.00316164\n",
      " -0.00316215]\n",
      "Weights for output layer\n",
      "[[-0.05639824]\n",
      " [-0.03736901]\n",
      " [ 0.02926476]\n",
      " ...\n",
      " [ 0.06035076]\n",
      " [ 0.0196325 ]\n",
      " [ 0.06448688]]\n",
      "Intercept for output layer\n",
      "[-0.00316227]\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(1024, activation='relu',input_shape=(64*64*3, )))\n",
    "network.add(layers.Dense(1024, activation='relu',input_shape=(64*64*3, )))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "train_images=x_train_final.reshape((128000, 64*64*3))\n",
    "train_images=train_images.astype('float32')/255\n",
    "\n",
    "test_images=x_test.reshape((50, 64*64*3))\n",
    "test_images=test_images.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels=y_train_final\n",
    "test_labels=y_test\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=1, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "print('test_loss:', test_loss)\n",
    "\n",
    "#weights for 1st hidden layer\n",
    "print('Weights for 1st hidden layer')\n",
    "print(network.layers[0].get_weights()[0])\n",
    "#intercept for 1st hidden layer\n",
    "print(\"Intercept for 1st hidden layer\")\n",
    "print(network.layers[0].get_weights()[1])\n",
    "\n",
    "#weights for 2nd hidden layer\n",
    "print('Weights for 2nd hidden layer')\n",
    "print(network.layers[1].get_weights()[0])\n",
    "#intercept for 2nd hidden layer\n",
    "print(\"Intercept for 1st hidden layer\")\n",
    "print(network.layers[1].get_weights()[1])\n",
    "\n",
    "#weights for output layer\n",
    "print('Weights for output layer')\n",
    "print(network.layers[2].get_weights()[0])\n",
    "#intercept for output layer\n",
    "print(\"Intercept for output layer\")\n",
    "print(network.layers[2].get_weights()[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
