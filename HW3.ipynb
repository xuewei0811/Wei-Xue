{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIREMENT    ROW 25\n",
    "\n",
    "#  For next Thursday, the assignment is as follows:  All teams\n",
    "# should use ReLU, inputs and outputs standardized to lie between 0 and 1,\n",
    "# the learning rate used in the last lecture (p. 16) with tau = 300, and\n",
    "# epsilon_0 = 0.01.  All of the teams should use the cross-entropy cost\n",
    "# function (p. 19).   All teams will train a network with two hidden\n",
    "# layers, each layer having 1,024 nodes.  We shall use the five data sets\n",
    "# from the first exercise:  MNIST digit (a), MNIST fashion (b), Cat/NonCat\n",
    "# (c), Cifar-10 (d), and Street View Digits (e).\n",
    "\n",
    "#         The experimental condition will be the use or not of weight\n",
    "# decay (p. 18).  Some groups will not use weight decay (condition 0),\n",
    "# others will use weight decay with a factor of 0.98 (condition 1).\n",
    "\n",
    "#        The assignment depends upon the last digit in your team's row in\n",
    "# the attached spreadsheet.\n",
    "\n",
    "# row ending \n",
    "\n",
    "# 1:  use dataset e, condition 0.\n",
    "\n",
    "# 2:  use d, condition 0.\n",
    "\n",
    "# 3:  c, 0\n",
    "\n",
    "# 4:  b, 0\n",
    "\n",
    "# 5:  a, 0\n",
    "\n",
    "# 6:  e, 1\n",
    "\n",
    "# 7:  d, 1\n",
    "\n",
    "# 8:  c, 1\n",
    "\n",
    "# 9:  b, 1\n",
    "\n",
    "# 0:  a, 1\n",
    "\n",
    "#        By next Thursday, upload your team's best accuracy on the test\n",
    "# data to the SAMSI website in a file named (your-row-number)hw2.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#set up seed\n",
    "#https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "from numpy.random import seed\n",
    "seed(23)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 00:04:07.961714 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 00:04:07.963645 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 00:04:07.966956 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 00:04:08.257076 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0917 00:04:08.265477 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0917 00:04:08.392879 4688504256 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 00:04:08.472015 4688504256 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 1.2760 - acc: 0.7548\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.6903 - acc: 0.8547\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.5787 - acc: 0.8673\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.5293 - acc: 0.8743\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.5003 - acc: 0.8780\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.4807 - acc: 0.8811\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.4663 - acc: 0.8831\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.4552 - acc: 0.8856\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.4463 - acc: 0.8868\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.4389 - acc: 0.8881\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.4327 - acc: 0.8891\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.4273 - acc: 0.8901\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.4226 - acc: 0.8905\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4185 - acc: 0.8911\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.4148 - acc: 0.8919\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4115 - acc: 0.8923\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.4085 - acc: 0.8928\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.4058 - acc: 0.8933\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.4032 - acc: 0.8937\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.4009 - acc: 0.8943\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3988 - acc: 0.8949\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.3967 - acc: 0.8951\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3949 - acc: 0.8954\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3931 - acc: 0.8958\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.3915 - acc: 0.8961\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3899 - acc: 0.8966\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3885 - acc: 0.8967\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3871 - acc: 0.8972\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3857 - acc: 0.8974\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3845 - acc: 0.8976\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3833 - acc: 0.8978\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3822 - acc: 0.8980\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3811 - acc: 0.8982\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3800 - acc: 0.8984\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3790 - acc: 0.8986\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3781 - acc: 0.8988\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3771 - acc: 0.8989\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.3763 - acc: 0.8991\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3754 - acc: 0.8992\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3746 - acc: 0.8993\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3738 - acc: 0.8996\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3730 - acc: 0.8997\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3723 - acc: 0.8997\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3715 - acc: 0.8999\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3708 - acc: 0.9000\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3702 - acc: 0.9002\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3695 - acc: 0.9002\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3689 - acc: 0.9004\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3683 - acc: 0.9006\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3677 - acc: 0.9007\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3671 - acc: 0.9007\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3665 - acc: 0.9009\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3659 - acc: 0.9009\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3654 - acc: 0.9009\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3649 - acc: 0.9011\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.3644 - acc: 0.9011\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3639 - acc: 0.9012\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3634 - acc: 0.9012\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3629 - acc: 0.9014\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3624 - acc: 0.9014\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3620 - acc: 0.9015\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3615 - acc: 0.9017\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3611 - acc: 0.9017\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3607 - acc: 0.9018\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3603 - acc: 0.9019\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3598 - acc: 0.9019\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.3594 - acc: 0.9020\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.3590 - acc: 0.9021\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.3587 - acc: 0.9021\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.3583 - acc: 0.9022\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3579 - acc: 0.9023\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3575 - acc: 0.9024\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3572 - acc: 0.9026\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.3568 - acc: 0.9026\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3565 - acc: 0.9027\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3562 - acc: 0.9027\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3558 - acc: 0.9028\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.3555 - acc: 0.9029\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3552 - acc: 0.9030\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3549 - acc: 0.9031\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3546 - acc: 0.9032\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3542 - acc: 0.9032\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3540 - acc: 0.9032\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3537 - acc: 0.9033\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3534 - acc: 0.9033\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3531 - acc: 0.9034\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3528 - acc: 0.9034\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3525 - acc: 0.9034\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3522 - acc: 0.9035\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3520 - acc: 0.9035\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3517 - acc: 0.9036\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3514 - acc: 0.9036\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3512 - acc: 0.9037\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3509 - acc: 0.9037\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3507 - acc: 0.9037\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.3504 - acc: 0.9038\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3502 - acc: 0.9038\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.3500 - acc: 0.9039\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.3497 - acc: 0.9039\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.3495 - acc: 0.9040\n",
      "10000/10000 [==============================] - 1s 99us/step\n",
      "test_acc: 0.9095\n",
      "Weights for 1st hidden layer\n",
      "[[-0.05126611 -0.02391104 -0.01311915 ...  0.00148087  0.0016106\n",
      "   0.04923698]\n",
      " [-0.04583436  0.00799055 -0.03536003 ... -0.02062291 -0.01358165\n",
      "   0.04999506]\n",
      " [-0.0225756  -0.01398358  0.02708543 ... -0.01278425 -0.01748106\n",
      "  -0.00634024]\n",
      " ...\n",
      " [-0.00749932  0.02094221  0.03328547 ...  0.02031729  0.03602517\n",
      "  -0.03927862]\n",
      " [ 0.04339685  0.04886182  0.03918439 ... -0.0057356   0.04879452\n",
      "  -0.03766268]\n",
      " [ 0.04595381 -0.00151692 -0.04332303 ... -0.01323945 -0.03896685\n",
      "  -0.030342  ]]\n",
      "Intercept for 1st hidden layer\n",
      "[ 0.00475539  0.00805282  0.00709491 ... -0.00042597 -0.00328934\n",
      "  0.01277407]\n",
      "Weights for 2nd hidden layer\n",
      "[[ 0.03493986 -0.0239275   0.05444817 ... -0.01431029  0.00977785\n",
      "   0.00836156]\n",
      " [-0.00450273  0.04229949  0.02521598 ... -0.01384617  0.02123718\n",
      "  -0.03392856]\n",
      " [-0.00319974  0.02462191  0.04823035 ...  0.04120895  0.00655712\n",
      "   0.04172211]\n",
      " ...\n",
      " [-0.03067102  0.02356034 -0.04642536 ...  0.00225466 -0.01681998\n",
      "   0.0225174 ]\n",
      " [ 0.02916247 -0.04936463  0.03368862 ...  0.01415859  0.04255179\n",
      "  -0.03772462]\n",
      " [ 0.04015871  0.01395485  0.02351404 ...  0.03305751  0.01306873\n",
      "   0.05600378]]\n",
      "Intercept for 1st hidden layer\n",
      "[ 0.00037683 -0.00455891  0.00377038 ...  0.01304639  0.00269935\n",
      "  0.00061631]\n",
      "Weights for output layer\n",
      "[[ 0.06201121  0.03513367  0.01473338 ...  0.04311559  0.01401381\n",
      "   0.02636176]\n",
      " [-0.00714626 -0.02602209  0.00280991 ... -0.0359928   0.0601607\n",
      "  -0.01620846]\n",
      " [ 0.02464798 -0.02632745 -0.06119913 ...  0.01136249 -0.07512425\n",
      "   0.03227365]\n",
      " ...\n",
      " [ 0.03871029  0.02109062 -0.08388508 ...  0.05070405 -0.05842981\n",
      "   0.09350433]\n",
      " [-0.00291924  0.03922573  0.00976131 ...  0.02067566  0.03115062\n",
      "  -0.02770679]\n",
      " [-0.06472564  0.02264651  0.04737021 ...  0.02141942  0.07516319\n",
      "  -0.03256856]]\n",
      "Intercept for output layer\n",
      "[-0.02822922  0.05491475 -0.01329328 -0.01202795  0.00808394  0.03979989\n",
      " -0.0061307   0.02126894 -0.06427745 -0.00010875]\n"
     ]
    }
   ],
   "source": [
    "#1.\tMNIST Dataset\n",
    "# http://yann.lecun.com/exdb/mnist/ \n",
    "#Import from Keras\n",
    "from keras.datasets import mnist\n",
    "#Setup train and test splits\n",
    "(x_train, y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(1024, activation='relu',input_shape=(28*28, )))\n",
    "network.add(layers.Dense(1024, activation='relu',input_shape=(28*28, )))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "train_images=x_train.reshape((60000, 28*28))\n",
    "train_images=train_images.astype('float32')/255\n",
    "\n",
    "test_images=x_test.reshape((10000, 28*28))\n",
    "test_images=test_images.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels=to_categorical(y_train)\n",
    "test_labels=to_categorical(y_test)\n",
    "\n",
    "\n",
    "epochs=100\n",
    "from keras.optimizers import SGD\n",
    "opt=SGD(lr=0.01, decay=0.99/(300-0.99*epochs))\n",
    "\n",
    "network.compile(optimizer=opt,\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=epochs, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "\n",
    "#weights for 1st hidden layer\n",
    "print('Weights for 1st hidden layer')\n",
    "print(network.layers[0].get_weights()[0])\n",
    "#intercept for 1st hidden layer\n",
    "print(\"Intercept for 1st hidden layer\")\n",
    "print(network.layers[0].get_weights()[1])\n",
    "\n",
    "#weights for 2nd hidden layer\n",
    "print('Weights for 2nd hidden layer')\n",
    "print(network.layers[1].get_weights()[0])\n",
    "#intercept for 2nd hidden layer\n",
    "print(\"Intercept for 1st hidden layer\")\n",
    "print(network.layers[1].get_weights()[1])\n",
    "\n",
    "#weights for output layer\n",
    "print('Weights for output layer')\n",
    "print(network.layers[2].get_weights()[0])\n",
    "#intercept for output layer\n",
    "print(\"Intercept for output layer\")\n",
    "print(network.layers[2].get_weights()[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
